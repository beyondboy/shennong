#!/usr/bin/env python
"""computes average ABX score

Computes average from csv files generated by `2_abx_score.sh` and
extract them as tables (.rst format) to be displayed in shennong
documentation

"""

import argparse
import collections
import joblib
import numpy as np
import os
import pandas
import tabulate


Entry = collections.namedtuple(
    'Entry', ['corpus', 'task', 'features', 'params', 'score'])


def average(df, task_type):
    if task_type == 'across':
        df['context'] = df['by']
    elif task_type == 'within':
        arr = np.array([eval(by) for by in df['by']])
        df['talker'] = [e for e, f in arr]
        df['context'] = [f for e, f in arr]
    else:
        raise ValueError('Unknown task type: {0}'.format(task_type))

    del df['by']

    # aggregate on talkers
    groups = df.groupby(['context', 'phone_1', 'phone_2'], as_index=False)
    df = groups['score'].mean()
    # aggregate on contexts
    groups = df.groupby(['phone_1', 'phone_2'], as_index=False)
    df = groups['score'].mean()

    return (1 - df.mean()[0]) * 100


def compute_scores(csv_files, scores_file, njobs=1):
    def _compute_score(csv):
        name = os.path.splitext(os.path.basename(csv))[0].split('_')
        task = name[0]
        score = average(pandas.read_csv(csv, sep='\t'), task)
        return Entry(
            corpus=name[1], task=name[0], features=name[2],
            params=name[3], score=score)

    entries = joblib.Parallel(n_jobs=njobs, verbose=10)(
        joblib.delayed(_compute_score)(csv) for csv in csv_files)

    return entries


def make_table(entries):
    def _get_entry(features, params):
        return [e for e in entries
                if e.features == features and e.params == params][0].score

    header = ['features', 'only', 'nocmvn', 'full']
    columns = header[1:]
    rows = sorted({e.features for e in entries})
    table = [[row] + [
        _get_entry(row, column) for column in columns] for row in rows]
    return tabulate.tabulate(
        table, header, tablefmt='rst', numalign='right', floatfmt='.1f')


def main():
    # parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('data_dir')
    parser.add_argument('-j', '--njobs', default=1, type=int)
    args = parser.parse_args()

    # setup input / output files
    scores_file = os.path.join(args.data_dir, 'final_scores.txt')

    # computes averaged scores and save them to a file
    abx_dir = os.path.join(args.data_dir, 'abx')
    csv_files = [
        os.path.join(abx_dir, f) for f in os.listdir(abx_dir)
        if f.endswith('.csv')]
    entries = compute_scores(csv_files, scores_file, njobs=args.njobs)
    with open(scores_file, 'w') as fout:
        for e in sorted(entries):
            fout.write('{} {} {} {} {}\n'.format(
                e.corpus, e.task, e.features, e.params, e.score))
    # # reload an already computed scores file
    # entries = [Entry._make(line.strip().split(' '))
    #            for line in open(scores_file, 'r')]

    # generates result tables from the scores (one table per corpus /
    # task combination)
    tables_dir = os.path.join(args.data_dir, 'tables')
    if not os.path.isdir(tables_dir):
        os.makedirs(tables_dir)
    for corpus in ('english', 'xitsonga'):
        for task in ('within', 'across'):
            table_entries = [
                e for e in entries if e.corpus == corpus and e.task == task]
            tfile = os.path.join(tables_dir, f'{corpus}_{task}.rst')
            open(tfile, 'w').write(make_table(table_entries) + '\n')


if __name__ == '__main__':
    main()
